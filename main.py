from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import pytesseract
from PIL import Image
import PyPDF2
import io
import os
from typing import Dict, List, Optional
from pydantic import BaseModel
import json
from datetime import datetime
import uuid
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class DocumentAnalysis(BaseModel):
    document_id: str
    filename: str
    text_content: str
    risks: List[Dict]
    benefits: List[Dict]
    unclear_terms: List[Dict]
    overall_rating: str  # "–±–µ–∑–æ–ø–∞—Å–µ–Ω", "—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è", "—Ä–∏—Å–∫–æ–≤–∞–Ω"
    summary: str
    processed_at: str

class ChatMessage(BaseModel):
    document_id: str
    question: str

class ChatResponse(BaseModel):
    answer: str
    document_id: str

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI(
    title="SafeDocs - AI –∑–∞—â–∏—Ç–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
    description="–ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
    version="1.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
os.makedirs("static", exist_ok=True)
os.makedirs("uploads", exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="static"), name="static")

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –±—É–¥–µ—Ç –ë–î)
documents_storage = {}
chat_history = {}

class FileProcessor:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
    
    @staticmethod
    def extract_text_from_pdf(file_content: bytes) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        try:
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text += f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n"
                        text += page_text + "\n"
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1}: {e}")
                    continue
            
            if not text.strip():
                raise Exception("PDF —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
                
            return text.strip()
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {str(e)}")
    
    @staticmethod
    def extract_text_from_image(file_content: bytes) -> str:
        """OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            image = Image.open(io.BytesIO(file_content))
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è OCR
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            width, height = image.size
            if width < 1000:
                scale = 1000 / width
                new_width = int(width * scale)
                new_height = int(height * scale)
                image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # OCR —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ
            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—èABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,;:!?()-_"‚Ññ%'
            text = pytesseract.image_to_string(image, lang='rus+eng', config=custom_config)
            
            if not text.strip():
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
                
            return text.strip()
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"–û—à–∏–±–∫–∞ OCR: {str(e)}")

class AIAnalyzer:
    """AI –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ–π, –ø–æ—Ç–æ–º Gemini)"""
    
    @staticmethod
    def analyze_document(text: str, filename: str) -> DocumentAnalysis:
        """–ê–Ω–∞–ª–∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        text_lower = text.lower()
        
        # –ü–æ–∏—Å–∫ —Ä–∏—Å–∫–æ–≤
        risks = []
        risk_patterns = {
            "—à—Ç—Ä–∞—Ñ–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏": ["—à—Ç—Ä–∞—Ñ", "–ø–µ–Ω—è", "–Ω–µ—É—Å—Ç–æ–π–∫–∞", "—Å–∞–Ω–∫—Ü–∏–∏"],
            "–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —É—Å–ª–æ–≤–∏—è": ["–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω", "–≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è", "–ø–æ —Å–≤–æ–µ–º—É —É—Å–º–æ—Ç—Ä–µ–Ω–∏—é", "–≤–ø—Ä–∞–≤–µ —Ä–∞—Å—Ç–æ—Ä–≥–Ω—É—Ç—å"],
            "–≤—ã—Å–æ–∫–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": ["–ø–æ–ª–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ –≤—Å–µ—Ö", "—Å–æ–ª–∏–¥–∞—Ä–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"],
            "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å—Ä–æ–∫–∏": ["—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫", "–≤ –∫—Ä–∞—Ç—á–∞–π—à–∏–µ —Å—Ä–æ–∫–∏", "–Ω–µ–∑–∞–º–µ–¥–ª–∏—Ç–µ–ª—å–Ω–æ", "–±–µ–∑ –ø—Ä–æ–º–µ–¥–ª–µ–Ω–∏—è"],
            "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–∏—Å–∫–∏": ["–∑–∞ —Å–≤–æ–π —Å—á–µ—Ç", "–±–µ–∑ –≤–æ–∑–º–µ—â–µ–Ω–∏—è", "–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ", "—É–±—ã—Ç–∫–∏ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è"]
        }
        
        for risk_type, keywords in risk_patterns.items():
            for keyword in keywords:
                if keyword in text_lower:
                    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    start_pos = text_lower.find(keyword)
                    context_start = max(0, start_pos - 100)
                    context_end = min(len(text), start_pos + 200)
                    context = text[context_start:context_end].strip()
                    
                    risks.append({
                        "type": risk_type,
                        "keyword": keyword,
                        "context": context,
                        "severity": "–≤—ã—Å–æ–∫–∏–π" if any(x in keyword for x in ["—à—Ç—Ä–∞—Ñ", "–ø–æ–ª–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"]) else "—Å—Ä–µ–¥–Ω–∏–π",
                        "recommendation": f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —É—Å–ª–æ–≤–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å '{keyword}'"
                    })
                    break  # –û–¥–∏–Ω —Ä–∏—Å–∫ –Ω–∞ —Ç–∏–ø
        
        # –ü–æ–∏—Å–∫ –≤—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
        benefits = []
        benefit_patterns = {
            "–≥–∞—Ä–∞–Ω—Ç–∏–∏": ["–≥–∞—Ä–∞–Ω—Ç–∏—è", "–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç", "–æ–±—è–∑—É–µ—Ç—Å—è"],
            "–∑–∞—â–∏—Ç–∞ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤": ["—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ"],
            "–≥–∏–±–∫–∏–µ —É—Å–ª–æ–≤–∏—è": ["–ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—é", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è", "–ø—Ä–∞–≤–æ –≤—ã–±–æ—Ä–∞"],
            "–≤–æ–∑–≤—Ä–∞—Ç —Å—Ä–µ–¥—Å—Ç–≤": ["–≤–æ–∑–≤—Ä–∞—Ç", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤"]
        }
        
        for benefit_type, keywords in benefit_patterns.items():
            for keyword in keywords:
                if keyword in text_lower:
                    start_pos = text_lower.find(keyword)
                    context_start = max(0, start_pos - 100)
                    context_end = min(len(text), start_pos + 200)
                    context = text[context_start:context_end].strip()
                    
                    benefits.append({
                        "type": benefit_type,
                        "keyword": keyword,
                        "context": context,
                        "value": "–≤—ã—Å–æ–∫–∞—è" if "–≥–∞—Ä–∞–Ω—Ç–∏—è" in keyword else "—Å—Ä–µ–¥–Ω—è—è"
                    })
                    break
        
        # –ü–æ–∏—Å–∫ –Ω–µ—è—Å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        unclear_terms = []
        unclear_patterns = [
            "–≤ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ", "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ä—ã", "–Ω–∞–¥–ª–µ–∂–∞—â–∏–º –æ–±—Ä–∞–∑–æ–º",
            "—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫", "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è", "—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞",
            "–∏–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞", "–ø–æ –æ–±–æ—é–¥–Ω–æ–º—É —Å–æ–≥–ª–∞—Å–∏—é", "–≤ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö"
        ]
        
        for pattern in unclear_patterns:
            if pattern in text_lower:
                start_pos = text_lower.find(pattern)
                context_start = max(0, start_pos - 100)
                context_end = min(len(text), start_pos + 200)
                context = text[context_start:context_end].strip()
                
                unclear_terms.append({
                    "phrase": pattern,
                    "context": context,
                    "explanation": f"–§—Ä–∞–∑–∞ '{pattern}' —Ç—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π",
                    "suggestion": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å–∏—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞"
                })
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        risk_count = len(risks)
        severe_risks = len([r for r in risks if r.get("severity") == "–≤—ã—Å–æ–∫–∏–π"])
        
        if severe_risks > 0:
            overall_rating = "—Ä–∏—Å–∫–æ–≤–∞–Ω"
        elif risk_count > 3:
            overall_rating = "—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        elif risk_count > 0:
            overall_rating = "—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        else:
            overall_rating = "–±–µ–∑–æ–ø–∞—Å–µ–Ω"
        
        # –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
        summary = f"""
–î–æ–∫—É–º–µ–Ω—Ç '{filename}' –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:

üîç –û–°–ù–û–í–ù–´–ï –ù–ê–•–û–î–ö–ò:
‚Ä¢ –†–∏—Å–∫–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(risks)}
‚Ä¢ –í—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π: {len(benefits)}
‚Ä¢ –ù–µ—è—Å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫: {len(unclear_terms)}

‚öñÔ∏è –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {overall_rating.upper()}

üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{"‚Ä¢ –û–±—Ä–∞—Ç–∏—Ç–µ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —à—Ç—Ä–∞—Ñ–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏" if any("—à—Ç—Ä–∞—Ñ" in r["keyword"] for r in risks) else ""}
{"‚Ä¢ –£—Ç–æ—á–Ω–∏—Ç–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏" if unclear_terms else ""}
{"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏ –≤ —Å–≤–æ—é –ø–æ–ª—å–∑—É" if benefits else ""}

üìã –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö. –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —é—Ä–∏—Å—Ç–æ–º.
        """.strip()
        
        return DocumentAnalysis(
            document_id=str(uuid.uuid4()),
            filename=filename,
            text_content=text[:2000] + "..." if len(text) > 2000 else text,
            risks=risks,
            benefits=benefits,
            unclear_terms=unclear_terms,
            overall_rating=overall_rating,
            summary=summary,
            processed_at=datetime.now().isoformat()
        )

# API Routes
@app.get("/")
async def get_main_page():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        return FileResponse('static/index.html')
    except FileNotFoundError:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        return {"message": "SafeDocs API —Ä–∞–±–æ—Ç–∞–µ—Ç. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª static/index.html –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."}

@app.post("/api/analyze", response_model=DocumentAnalysis)
async def analyze_document(file: UploadFile = File(...)):
    """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
    allowed_types = {
        'application/pdf': 'pdf',
        'image/jpeg': 'image',
        'image/jpg': 'image', 
        'image/png': 'image',
        'text/plain': 'text'
    }
    
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400, 
            detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: PDF, JPG, PNG, TXT"
        )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–º–∞–∫—Å 10MB)
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    file_content = await file.read()
    
    if len(file_content) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å 10MB)")
    
    if len(file_content) == 0:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –ø—É—Å—Ç–æ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    file_path = f"uploads/{uuid.uuid4()}_{file.filename}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    
    file_type = allowed_types[file.content_type]
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    try:
        if file_type == 'pdf':
            text = FileProcessor.extract_text_from_pdf(file_content)
        elif file_type == 'image':
            text = FileProcessor.extract_text_from_image(file_content)
        elif file_type == 'text':
            text = file_content.decode('utf-8')
        
        if len(text.strip()) < 50:
            raise HTTPException(
                status_code=400, 
                detail="–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤)"
            )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        analysis = AIAnalyzer.analyze_document(text, file.filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        documents_storage[analysis.document_id] = analysis
        
        return analysis
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_document(message: ChatMessage):
    """–ß–∞—Ç —Å AI –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
    
    if message.document_id not in documents_storage:
        raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    document = documents_storage[message.document_id]
    
    # –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç (–ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ Gemini)
    question_lower = message.question.lower()
    
    # –ì–æ—Ç–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    if any(word in question_lower for word in ["—Ä–∏—Å–∫", "–æ–ø–∞—Å–Ω", "—à—Ç—Ä–∞—Ñ"]):
        risks_text = "\n".join([f"‚Ä¢ {r['type']}: {r['recommendation']}" for r in document.risks])
        answer = f"üö® –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–∏—Å–∫–∏:\n\n{risks_text}" if risks_text else "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–µ—Ä—å–µ–∑–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤."
    
    elif any(word in question_lower for word in ["–≤—ã–≥–æ–¥", "–ø–ª—é—Å", "—Ö–æ—Ä–æ—à"]):
        benefits_text = "\n".join([f"‚Ä¢ {b['type']}: {b['keyword']}" for b in document.benefits])
        answer = f"‚úÖ –í—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:\n\n{benefits_text}" if benefits_text else "–Ø–≤–Ω—ã—Ö –≤—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    elif any(word in question_lower for word in ["–Ω–µ–ø–æ–Ω—è—Ç–Ω", "–Ω–µ—è—Å–Ω", "—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç"]):
        unclear_text = "\n".join([f"‚Ä¢ {u['phrase']}: {u['explanation']}" for u in document.unclear_terms])
        answer = f"‚ùì –ù–µ—è—Å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏:\n\n{unclear_text}" if unclear_text else "–í—Å–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–Ω—è—Ç–Ω—ã."
    
    elif any(word in question_lower for word in ["–ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å", "—Å–æ–≥–ª–∞—Å–∏—Ç—å—Å—è", "—Å—Ç–æ–∏—Ç –ª–∏"]):
        answer = f"""
ü§î –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É:

–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {document.overall_rating}

{"‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É—é –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å —Ä–∏—Å–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ–¥–ø–∏—Å–∞–Ω–∏–µ–º." if document.overall_rating != "–±–µ–∑–æ–ø–∞—Å–µ–Ω" else "‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –≤—ã–≥–ª—è–¥–∏—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º."}

üí° –°–æ–≤–µ—Ç—É—é:
1. –ü—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —é—Ä–∏—Å—Ç–æ–º
2. –û–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
3. –£—Ç–æ—á–Ω–∏—Ç—å –Ω–µ—è—Å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

–ü–æ–º–Ω–∏—Ç–µ: —ç—Ç–æ –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑, –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞ –≤–∞–º–∏!
        """
    
    else:
        answer = f"""
üí¨ –°–ø–∞—Å–∏–±–æ –∑–∞ –≤–æ–ø—Ä–æ—Å! 

–Ø –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –≤–∞—à –¥–æ–∫—É–º–µ–Ω—Ç "{document.filename}" –∏ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ:
‚Ä¢ –†–∏—Å–∫–∞—Ö –≤ –¥–æ–≥–æ–≤–æ—Ä–µ
‚Ä¢ –í—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö  
‚Ä¢ –ù–µ—è—Å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö
‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –ø–æ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—é

–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å: "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∏—Å–∫–∏?" –∏–ª–∏ "–°—Ç–æ–∏—Ç –ª–∏ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å?"

üî¨ –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π AI-–ø–æ–º–æ—â–Ω–∏–∫ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞.
        """
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
    if message.document_id not in chat_history:
        chat_history[message.document_id] = []
    
    chat_history[message.document_id].append({
        "question": message.question,
        "answer": answer,
        "timestamp": datetime.now().isoformat()
    })
    
    return ChatResponse(answer=answer, document_id=message.document_id)

@app.get("/api/documents")
async def get_documents():
    """–°–ø–∏—Å–æ–∫ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return {
        "documents": [
            {
                "id": doc_id,
                "filename": doc.filename,
                "overall_rating": doc.overall_rating,
                "processed_at": doc.processed_at,
                "risks_count": len(doc.risks),
                "benefits_count": len(doc.benefits)
            }
            for doc_id, doc in documents_storage.items()
        ]
    }

@app.get("/api/documents/{document_id}")
async def get_document(document_id: str):
    """–î–µ—Ç–∞–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    if document_id not in documents_storage:
        raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    return documents_storage[document_id]

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return {
        "status": "healthy",
        "service": "SafeDocs",
        "version": "1.0.0",
        "documents_count": len(documents_storage)
    }

if __name__ == "__main__":
    import uvicorn
    
    print("üõ°Ô∏è  SafeDocs –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    print("üåê –í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: http://localhost:8000")
    print("üìö API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
    print("üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API: http://localhost:8000/redoc")
    
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)